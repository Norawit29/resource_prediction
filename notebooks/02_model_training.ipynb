{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPyrvR8krAFWj/cDRi50rTM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Norawit29/resource_prediction/blob/main/02_model_training_ipynbipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2. Model Training and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "849OIJuqog7B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from scipy.stats import randint, uniform\n",
        "import pickle\n",
        "\n",
        "\n",
        "# Model Training and Hyperparameter Tuning\n",
        "# Define the models\n",
        "models = {\n",
        "    'LinearRegression': LinearRegression(),\n",
        "    'Ridge': Ridge(),\n",
        "    'Lasso': Lasso(),\n",
        "    'ElasticNet': ElasticNet(),\n",
        "    'RandomForest': RandomForestRegressor(),\n",
        "    'XGBoost': XGBRegressor(),\n",
        "    'LightGBM': LGBMRegressor()\n",
        "}\n",
        "\n",
        "# Hyperparameter space for each algorithm\n",
        "hyperparameter_space = {\n",
        "    'Ridge': {\n",
        "        'model__alpha': [0.1, 1, 10, 100, 1000]\n",
        "    },\n",
        "    'Lasso': {\n",
        "        'model__alpha': [0.1, 1, 10, 100, 1000]\n",
        "    },\n",
        "    'ElasticNet': {\n",
        "        'model__alpha': [0.1, 1, 10, 100, 1000],\n",
        "        'model__l1_ratio': [0.1, 0.5, 0.9]\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'model__n_estimators': randint(100, 1000),\n",
        "        'model__max_features': ['auto', 'sqrt'],\n",
        "        'model__max_depth': randint(10, 100),\n",
        "        'model__min_samples_split': randint(2, 10),\n",
        "        'model__min_samples_leaf': randint(1, 4),\n",
        "        'model__bootstrap': [True, False]\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model__n_estimators': randint(100, 1000),\n",
        "        'model__learning_rate': uniform(0.01, 0.3),\n",
        "        'model__max_depth': randint(3, 10),\n",
        "        'model__min_child_weight': randint(1, 10),\n",
        "        'model__subsample': uniform(0.6, 0.4),\n",
        "        'model__colsample_bytree': uniform(0.5, 0.5)\n",
        "    },\n",
        "    'LightGBM': {\n",
        "        'model__n_estimators': randint(100, 1000),\n",
        "        'model__learning_rate': uniform(0.01, 0.3),\n",
        "        'model__num_leaves': randint(20, 50),\n",
        "        'model__max_depth': randint(3, 10),\n",
        "        'model__min_child_samples': randint(10, 30),\n",
        "        'model__subsample': uniform(0.6, 0.4)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Training and tuning each model\n",
        "best_models = {}\n",
        "for model_name, model in models.items():\n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    if model_name in hyperparameter_space:\n",
        "        # Perform Randomized Search\n",
        "        random_search = RandomizedSearchCV(\n",
        "            pipeline,\n",
        "            param_distributions=hyperparameter_space[model_name],\n",
        "            n_iter=10,\n",
        "            cv=5,\n",
        "            verbose=2,\n",
        "            random_state=42,\n",
        "            n_jobs=-1,\n",
        "            scoring='neg_root_mean_squared_error'\n",
        "        )\n",
        "        random_search.fit(X_train, y_train)\n",
        "        best_models[model_name] = random_search.best_estimator_\n",
        "        print(f\"Best parameters for {model_name}: {random_search.best_params_}\")\n",
        "        print(f\"Best score (negative RMSE) for {model_name}: {random_search.best_score_}\")\n",
        "\n",
        "    else:\n",
        "        # Fit model without hyperparameter tuning\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        best_models[model_name] = pipeline\n",
        "\n",
        "     # Save the best model for each algorithm\n",
        "    with open(f'best_model_{model_name}.pkl', 'wb') as file:\n",
        "        pickle.dump(best_model, file)\n",
        "    print(f\"Best model for {model_name} saved as 'best_model_{model_name}.pkl'\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Nzui0_LjoiQ8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
